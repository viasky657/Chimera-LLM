Steps to Train Sonar Model List (TO-DO List)
1. Add Images and Videos to abstract Conceptual learning Model with Binary encoder (Meta) instead of autotransformer (Sonar Encoder and Decoder). (Added code needed for training but need to train)
2. Add Gated Autoregressive encoder for the Sonar training to reduce gradiant learning knowledge saving errors. 
3. Maybe add some of the audio learning dataset from Qwen Audio for understanding multiple speakers?

Steps to Train LCM (Large Concept Model) (Meta), LBM (Large Brain Model), B-Star (Model that can self-train and encourage exploration), Memory Layers (Meta) (Optmimized Self-Learning and Long Term Memory Storage), Binary Encoder (Meta), Flaming Filter (Meta):
1. Add Brain LLM Arch to the LCM Long Term Memory (Transformer Arch) replacement. (Started but not complete)
2. Add B-Star Model Arch to the Transformer Long Term Memory Part. 
3. Add Memory Layers to the Transformer Long Term Memory Part. 
4. Add Flaming Filter (if possible) at this step. 
5. Add Binary encoder instead of autotokenizer.
6. Add New Sonar model with additional training to the pipeline and replace the old sonar model. 
7. Train the LLM on MRI and EEG datasets.
8. Add training for Opengpt dataset and the Pile dataset and maybe some other dataset if time(?)
9. Add training for model Introspection for enhanced accuracy and preformance: https://arxiv.org/abs/2410.13787.
10. Add architecture for self-evolving LLM by saving the tokens to the memory layer (meta) instead of the one suggested by the self-evolving paper: https://writer.com/engineering/self-evolving-models/.

Improvements
1. Add complex thinking training datasets
2. EEG and Free Will Study(?)

