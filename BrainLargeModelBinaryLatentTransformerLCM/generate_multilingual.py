import torch
import torch.nn.functional as F
from character_aware_blt import CharacterAwareBLT
from pathlib import Path
import numpy as np
from typing import List, Dict, Optional
import unicodedata
import json
from tqdm import tqdm

class MultilingualGenerator:
    """
    Generates text using the SONAR BLT model with:
    1. Multi-script awareness
    2. Character-level control
    3. Language-specific patterns
    4. Cross-lingual capabilities
    """
    def __init__(
        self,
        model_path: str,
        data_dir: str,
        device: torch.device = None
    ):
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
        
        # Load model
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.model = CharacterAwareBLT(
            d_model=512,
            n_layers=24,
            n_heads=8,
            encoder_layers=1,
            decoder_layers=9,
            window_size=512,
            max_ngram=8,
            hash_vocab_size=300000,
            dropout=0.1,
            paragraph_dim=1024
        ).to(self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        # Load script statistics
        self.data_dir = Path(data_dir)
        with open(self.data_dir / 'script_stats.json') as f:
            self.script_stats = json.load(f)
    
    def generate_with_script(
        self,
        prompt: str,
        target_script: str,
        max_length: int = 1024,
        temperature: float = 0.8,
        top_p: float = 0.9
    ) -> str:
        """Generate text in a specific script"""
        # Convert prompt to bytes
        bytes_seq = torch.tensor(
            [b for b in prompt.encode()],
            dtype=torch.long
        ).unsqueeze(0).to(self.device)
        
        # Get script character statistics
        script_chars = set()
        for script, stats in self.script_stats.items():
            if script.startswith(target_script):
                script_chars.update(stats['example_chars'])
        
        # Generate with script awareness
        generated = []
        
        with torch.no_grad():
            for _ in range(max_length):
                # Get hierarchical predictions
                logits, hierarchical = self.model(
                    bytes_seq,
                    return_hierarchical=True
                )
                
                # Get next byte probabilities
                probs = F.softmax(logits[:, -1] / temperature, dim=-1)
                
                # Modify probabilities based on script
                char_weights = self.get_script_weights(
                    hierarchical,
                    script_chars
                )
                probs = probs * char_weights
                probs = probs / probs.sum()
                
                # Apply nucleus sampling
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(
                    1, sorted_indices, sorted_indices_to_remove
                )
                probs[indices_to_remove] = 0
                probs = probs / probs.sum()
                
                # Sample next byte
                next_byte = torch.multinomial(probs, 1)
                
                # Append to sequence
                bytes_seq = torch.cat([bytes_seq, next_byte], dim=1)
                generated.append(next_byte.item())
                
                # Check for end condition
                if next_byte.item() == 0:
                    break
        
        # Convert generated bytes to text
        try:
            return bytes(generated).decode('utf-8')
        except UnicodeDecodeError:
            return bytes(generated).decode('utf-8', errors='replace')
    
    def generate_with_style(
        self,
        prompt: str,
        style_text: str,
        max_length: int = 1024,
        temperature: float = 0.8,
        top_p: float = 0.9
    ) -> str:
        """Generate text matching the style of a reference text"""
        # Analyze style text
        style_chars = set(style_text)
        style_patterns = self.analyze_text_patterns(style_text)
        
        # Convert prompt to bytes
        bytes_seq = torch.tensor(
            [b for b in prompt.encode()],
            dtype=torch.long
        ).unsqueeze(0).to(self.device)
        
        # Generate with style matching
        generated = []
        current_pattern = []
        
        with torch.no_grad():
            for _ in range(max_length):
                # Get predictions
                logits, hierarchical = self.model(
                    bytes_seq,
                    return_hierarchical=True
                )
                
                # Get next byte probabilities
                probs = F.softmax(logits[:, -1] / temperature, dim=-1)
                
                # Modify probabilities based on style
                style_weights = self.get_style_weights(
                    hierarchical,
                    style_chars,
                    style_patterns,
                    current_pattern
                )
                probs = probs * style_weights
                probs = probs / probs.sum()
                
                # Apply nucleus sampling
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(
                    1, sorted_indices, sorted_indices_to_remove
                )
                probs[indices_to_remove] = 0
                probs = probs / probs.sum()
                
                # Sample next byte
                next_byte = torch.multinomial(probs, 1)
                
                # Update sequences
                bytes_seq = torch.cat([bytes_seq, next_byte], dim=1)
                generated.append(next_byte.item())
                
                # Update current pattern
                try:
                    char = bytes([next_byte.item()]).decode('utf-8')
                    current_pattern.append(char)
                    if len(current_pattern) > 5:  # Keep pattern window
                        current_pattern.pop(0)
                except UnicodeDecodeError:
                    pass
                
                # Check for end condition
                if next_byte.item() == 0:
                    break
        
        # Convert generated bytes to text
        try:
            return bytes(generated).decode('utf-8')
        except UnicodeDecodeError:
            return bytes(generated).decode('utf-8', errors='replace')
    
    def generate_translation(
        self,
        text: str,
        source_script: str,
        target_script: str,
        max_length: int = 1024,
        temperature: float = 0.8,
        top_p: float = 0.9
    ) -> str:
        """Generate text translated between scripts"""
        # Get script character mappings
        source_chars = set()
        target_chars = set()
        
        for script, stats in self.script_stats.items():
            if script.startswith(source_script):
                source_chars.update(stats['example_chars'])
            if script.startswith(target_script):
                target_chars.update(stats['example_chars'])
        
        # Convert input to bytes
        bytes_seq = torch.tensor(
            [b for b in text.encode()],
            dtype=torch.long
        ).unsqueeze(0).to(self.device)
        
        # Get source text representation
        with torch.no_grad():
            _, source_hier = self.model(
                bytes_seq,
                return_hierarchical=True
            )
        
        # Generate translation
        generated = []
        source_pos = 0
        
        with torch.no_grad():
            for _ in range(max_length):
                # Get predictions
                logits, target_hier = self.model(
                    bytes_seq,
                    return_hierarchical=True
                )
                
                # Get next byte probabilities
                probs = F.softmax(logits[:, -1] / temperature, dim=-1)
                
                # Modify probabilities based on translation
                trans_weights = self.get_translation_weights(
                    source_hier,
                    target_hier,
                    source_chars,
                    target_chars,
                    source_pos
                )
                probs = probs * trans_weights
                probs = probs / probs.sum()
                
                # Apply nucleus sampling
                sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = 0
                indices_to_remove = sorted_indices_to_remove.scatter(
                    1, sorted_indices, sorted_indices_to_remove
                )
                probs[indices_to_remove] = 0
                probs = probs / probs.sum()
                
                # Sample next byte
                next_byte = torch.multinomial(probs, 1)
                
                # Update sequences
                bytes_seq = torch.cat([bytes_seq, next_byte], dim=1)
                generated.append(next_byte.item())
                source_pos += 1
                
                # Check for end condition
                if next_byte.item() == 0 or source_pos >= len(text):
                    break
        
        # Convert generated bytes to text
        try:
            return bytes(generated).decode('utf-8')
        except UnicodeDecodeError:
            return bytes(generated).decode('utf-8', errors='replace')
    
    def get_script_weights(
        self,
        hierarchical: Dict,
        script_chars: set
    ) -> torch.Tensor:
        """Calculate weights to bias generation toward script"""
        weights = torch.ones(256, device=self.device)
        
        # Get character representations
        char_reprs = []
        for i in range(len(hierarchical['char_boundaries'])-1):
            start = hierarchical['char_boundaries'][i]
            end = hierarchical['char_boundaries'][i+1]
            char_reprs.append(
                hierarchical['characters'][start:end].mean(0)
            )
        
        if not char_reprs:
            return weights
        
        # Get script character representations
        script_reprs = []
        for char in script_chars:
            try:
                char_bytes = torch.tensor(
                    [b for b in char.encode('utf-8')],
                    dtype=torch.long
                ).unsqueeze(0).to(self.device)
                
                with torch.no_grad():
                    _, hier = self.model(char_bytes, return_hierarchical=True)
                    script_reprs.append(hier['characters'].mean(0))
            except:
                continue
        
        if not script_reprs:
            return weights
        
        # Calculate similarity-based weights
        script_reprs = torch.stack(script_reprs)
        last_char = torch.stack(char_reprs)[-1]
        
        for i in range(256):
            sims = F.cosine_similarity(
                last_char,
                script_reprs,
                dim=0
            )
            weights[i] *= (1 + sims.mean()) / 2
        
        return weights
    
    def analyze_text_patterns(self, text: str) -> Dict[str, float]:
        """Analyze character patterns in text"""
        patterns = defaultdict(int)
        total = 0
        
        # Count character n-grams
        for n in range(1, 6):
            for i in range(len(text) - n + 1):
                pattern = text[i:i+n]
                patterns[pattern] += 1
                total += 1
        
        # Convert to probabilities
        return {
            pattern: count / total
            for pattern, count in patterns.items()
        }
    
    def get_style_weights(
        self,
        hierarchical: Dict,
        style_chars: set,
        style_patterns: Dict[str, float],
        current_pattern: List[str]
    ) -> torch.Tensor:
        """Calculate weights to match text style"""
        weights = torch.ones(256, device=self.device)
        
        # Character preference
        char_weights = self.get_script_weights(
            hierarchical,
            style_chars
        )
        weights *= char_weights
        
        # Pattern matching
        if current_pattern:
            pattern = ''.join(current_pattern)
            for next_char in map(chr, range(256)):
                next_pattern = pattern + next_char
                if next_pattern in style_patterns:
                    weights[ord(next_char)] *= (1 + style_patterns[next_pattern])
        
        return weights
    
    def get_translation_weights(
        self,
        source_hier: Dict,
        target_hier: Dict,
        source_chars: set,
        target_chars: set,
        position: int
    ) -> torch.Tensor:
        """Calculate weights for script translation"""
        weights = torch.ones(256, device=self.device)
        
        # Get source character at position
        if position < len(source_hier['char_boundaries']) - 1:
            start = source_hier['char_boundaries'][position]
            end = source_hier['char_boundaries'][position + 1]
            source_repr = source_hier['characters'][start:end].mean(0)
            
            # Find similar characters in target script
            for char in target_chars:
                try:
                    char_bytes = torch.tensor(
                        [b for b in char.encode('utf-8')],
                        dtype=torch.long
                    ).unsqueeze(0).to(self.device)
                    
                    with torch.no_grad():
                        _, hier = self.model(char_bytes, return_hierarchical=True)
                        char_repr = hier['characters'].mean(0)
                        
                        sim = F.cosine_similarity(
                            source_repr,
                            char_repr,
                            dim=0
                        )
                        
                        for b in char.encode('utf-8'):
                            weights[b] *= (1 + sim) / 2
                except:
                    continue
        
        return weights

def main():
    # Initialize generator
    generator = MultilingualGenerator(
        model_path='best_sonar_blt_model.pt',
        data_dir='sonar_character_data'
    )
    
    # Example 1: Generate in specific script
    print("\nGenerating Latin script text:")
    latin_text = generator.generate_with_script(
        prompt="The quick brown ",
        target_script="LATIN",
        max_length=100
    )
    print(latin_text)
    
    # Example 2: Generate with style matching
    print("\nGenerating with style matching:")
    style_text = """
    In ancient times, beneath the starlit sky,
    Where wisdom flows and legends never die,
    The sages spoke of mysteries untold,
    Of sacred truths and prophecies of old.
    """
    styled_text = generator.generate_with_style(
        prompt="In the depths of time,",
        style_text=style_text,
        max_length=100
    )
    print(styled_text)
    
    # Example 3: Generate script translation
    print("\nGenerating script translation:")
    translated = generator.generate_translation(
        text="Hello World",
        source_script="LATIN",
        target_script="CYRILLIC",
        max_length=100
    )
    print(translated)

if __name__ == "__main__":
    main()
